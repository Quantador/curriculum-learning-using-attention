{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d92d4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8319cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cfg:\n",
    "    model_dim = 256\n",
    "    n_heads   = 4\n",
    "    n_layers  = 2\n",
    "    ff_dim    = 1024\n",
    "    dropout   = 0.1\n",
    "    seq_len   = 128\n",
    "    vocab     = None  # set after tokenizer loads\n",
    "\n",
    "    pool_multiplier = 4\n",
    "    sel_k     = 8     # selected subset (k <= M)\n",
    "\n",
    "    pool_M    = pool_multiplier * sel_k\n",
    "    \n",
    "    temp      = 1.0   # router softmax temperature\n",
    "    lambda_ent = 1e-2 # entropy regularization\n",
    "\n",
    "    batch_steps = 2000\n",
    "    lr       = 3e-4\n",
    "    wd       = 0.1\n",
    "    device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    val_bs   = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "63ff6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wikitext2(tokenizer, split=\"train\", seq_len=128):\n",
    "    # Concatenate all text and chunk into fixed-length token sequences\n",
    "    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")[split]\n",
    "    text = \"\\n\\n\".join(ds[\"text\"])\n",
    "    toks = tokenizer(text, return_tensors=None, add_special_tokens=False)[\"input_ids\"]\n",
    "    # make length divisible\n",
    "    n = (len(toks) // (seq_len+1)) * (seq_len+1)\n",
    "    toks = toks[:n]\n",
    "    x = torch.tensor(toks, dtype=torch.long).view(-1, seq_len+1)  # [N, L+1]\n",
    "    # inputs/labels (next-token)\n",
    "    return x[:, :-1], x[:, 1:]  # [N, L], [N, L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d4b657da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDatasetSimple(Dataset):\n",
    "    def __init__(self, x, y): self.x, self.y = x, y\n",
    "    def __len__(self): return self.x.size(0)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ef5fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyCausalLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d=256, n_heads=4, n_layers=2, ff=1024, dropout=0.1, seq_len=128):\n",
    "        super().__init__()\n",
    "        self.tok = nn.Embedding(vocab_size, d)\n",
    "        self.pos = nn.Embedding(seq_len, d)\n",
    "        layer = nn.TransformerEncoderLayer(d_model=d, nhead=n_heads,\n",
    "                                           dim_feedforward=ff, dropout=dropout,\n",
    "                                           batch_first=True, activation=\"gelu\")\n",
    "        self.enc  = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.ln   = nn.LayerNorm(d)\n",
    "        self.head = nn.Linear(d, vocab_size, bias=False)\n",
    "        self.seq_len = seq_len\n",
    "        self.d = d\n",
    "\n",
    "    def causal_mask(self, L):\n",
    "        # (L, L) mask with -inf above diagonal\n",
    "        m = torch.full((L, L), float(\"-inf\"), device=self.pos.weight.device)\n",
    "        return torch.triu(m, diagonal=1)\n",
    "\n",
    "    def forward(self, x):  # x: [B, L]\n",
    "        B, L = x.shape\n",
    "        pos = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        h = self.tok(x) + self.pos(pos)\n",
    "        logits = self.enc(h, mask=self.causal_mask(L))\n",
    "        logits = self.ln(logits)\n",
    "        return self.head(logits)  # [B, L, V]\n",
    "\n",
    "    def nll(self, x, y):\n",
    "        logits = self.forward(x)\n",
    "        return F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ppl(self, loader):\n",
    "        self.eval()\n",
    "        tot_loss, tot_tok = 0.0, 0\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(self.pos.weight.device), y.to(self.pos.weight.device)\n",
    "            loss = self.nll(x, y)\n",
    "            tot_loss += loss.item() * y.numel()\n",
    "            tot_tok  += y.numel()\n",
    "        self.train()\n",
    "        return math.exp(tot_loss / max(1, tot_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fd00a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention over samples: each sample j has a key k_j from its\n",
    "    (mean-pooled) token embeddings. A learned query q attends to k_j\n",
    "    to produce scores; softmax → probabilities over samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, d_attn=128):\n",
    "        super().__init__()\n",
    "        self.Wk = nn.Linear(d_in, d_attn, bias=False)  # keys from sample embeddings\n",
    "        self.q  = nn.Parameter(torch.randn(d_attn))    # learned global query\n",
    "        nn.init.normal_(self.q, std=0.02)\n",
    "\n",
    "    def forward(self, sample_embs, temp=1.0):\n",
    "        # sample_embs: [M, d_in] (per-sample embedding)\n",
    "        K = self.Wk(sample_embs)                # [M, d_attn]\n",
    "        q = self.q / (K.size(-1) ** 0.5)        # scale like dot-attention\n",
    "        scores = (K @ q)                        # [M]\n",
    "        probs = F.softmax(scores / temp, dim=0) # [M]\n",
    "        return probs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "656b18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_embeddings_from_tokens(lm: TinyCausalLM, X: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Produce per-sample embeddings cheaply:\n",
    "    mean-pool the token embeddings only (no encoder) to keep it simple/fast.\n",
    "    X: [M, L] → returns [M, d]\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        tok = lm.tok(X)           # [M, L, d]\n",
    "        embs = tok.mean(dim=1)    # [M, d]\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e89a5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(cfg, lm, router, pool_batch, pool_indices, opt):\n",
    "    device = cfg.device\n",
    "    X = torch.stack([xy[0] for xy in pool_batch]).to(device)\n",
    "    Y = torch.stack([xy[1] for xy in pool_batch]).to(device)\n",
    "\n",
    "    # router\n",
    "    sample_embs = sample_embeddings_from_tokens(lm, X)\n",
    "    probs, _ = router(sample_embs, temp=cfg.temp)\n",
    "    k = min(cfg.sel_k, cfg.pool_M)\n",
    "    idx = torch.topk(probs, k=k, dim=0).indices        # [k]\n",
    "\n",
    "    sel_x = X.index_select(0, idx)\n",
    "    sel_y = Y.index_select(0, idx)\n",
    "    loss_lm = lm.nll(sel_x, sel_y)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = lm(sel_x)\n",
    "        ce = F.cross_entropy(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            sel_y.reshape(-1),\n",
    "            reduction=\"none\",\n",
    "        ).reshape(k, -1)\n",
    "        per_sample_loss = ce.mean(dim=1)               # [k]\n",
    "        sel_mean_loss = per_sample_loss.mean().item()\n",
    "\n",
    "        # small random control inside the same pool (no update)\n",
    "        ctrl_k = min( max(1, k//2), X.size(0)-k )\n",
    "        # pick control indices disjoint from selected\n",
    "        mask = torch.ones(X.size(0), dtype=torch.bool, device=device)\n",
    "        mask[idx] = False\n",
    "        ctrl_all = torch.nonzero(mask, as_tuple=False).squeeze(1)\n",
    "        ctrl_idx = ctrl_all[torch.randperm(ctrl_all.numel(), device=device)[:ctrl_k]]\n",
    "\n",
    "        ctrl_x = X.index_select(0, ctrl_idx)\n",
    "        ctrl_y = Y.index_select(0, ctrl_idx)\n",
    "        ctrl_logits = lm(ctrl_x)\n",
    "        ctrl_ce = F.cross_entropy(\n",
    "            ctrl_logits.reshape(-1, ctrl_logits.size(-1)),\n",
    "            ctrl_y.reshape(-1),\n",
    "            reduction=\"none\",\n",
    "        ).reshape(ctrl_k, -1)\n",
    "        ctrl_mean_loss = ctrl_ce.mean(dim=1).mean().item()\n",
    "\n",
    "        R = per_sample_loss\n",
    "        Rn = (R - R.mean()) / (R.std() + 1e-8)\n",
    "\n",
    "    logp_selected = (probs.index_select(0, idx).clamp_min(1e-12)).log()\n",
    "    loss_router = -(Rn.detach() * logp_selected).sum()\n",
    "    loss_ent = cfg.lambda_ent * (-(probs * (probs.clamp_min(1e-12)).log()).sum())\n",
    "    loss = loss_lm + loss_router + loss_ent\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(lm.parameters(), 1.0)\n",
    "    torch.nn.utils.clip_grad_norm_(router.parameters(), 1.0)\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ent = (-(probs * (probs.clamp_min(1e-12)).log()).sum()).item()\n",
    "        selected_global = [pool_indices[i.item()] for i in idx]\n",
    "\n",
    "    return {\n",
    "        \"loss_lm\": loss_lm.item(),\n",
    "        \"loss_router\": loss_router.item(),\n",
    "        \"entropy\": ent,\n",
    "        \"sel_mean_loss\": sel_mean_loss,\n",
    "        \"ctrl_mean_loss\": ctrl_mean_loss,\n",
    "        \"selected_indices\": selected_global\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc609863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/wikitext.py'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/.huggingface.yaml'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/dataset_infos.json'\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2428601 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/wikitext.py'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/.huggingface.yaml'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/dataset_infos.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device=cuda | WT2 samples: train=18826 val=1946 | pool M=32, k=8\n",
      "[0050] LM=7.984  Router=-0.009  Ent=3.466  PPL=2339.42  Tokens=0.05M  Overlap=0.00  Coverage=2.07%  Skew(CV)=0.15  SelLoss=7.968  CtrlLoss=7.789\n",
      "[0100] LM=7.617  Router=-0.019  Ent=3.466  PPL=1455.41  Tokens=0.10M  Overlap=0.00  Coverage=4.08%  Skew(CV)=0.20  SelLoss=7.615  CtrlLoss=7.122\n",
      "[0150] LM=7.421  Router=-0.022  Ent=3.466  PPL=1314.34  Tokens=0.15M  Overlap=0.00  Coverage=5.91%  Skew(CV)=0.27  SelLoss=7.427  CtrlLoss=7.100\n",
      "[0200] LM=7.359  Router=-0.019  Ent=3.466  PPL=1207.90  Tokens=0.20M  Overlap=0.00  Coverage=7.63%  Skew(CV)=0.32  SelLoss=7.358  CtrlLoss=6.896\n",
      "[0250] LM=7.269  Router=-0.062  Ent=3.466  PPL=1138.81  Tokens=0.26M  Overlap=0.00  Coverage=9.33%  Skew(CV)=0.33  SelLoss=7.275  CtrlLoss=6.868\n",
      "[0300] LM=7.206  Router=-0.039  Ent=3.466  PPL=1096.35  Tokens=0.31M  Overlap=0.00  Coverage=10.92%  Skew(CV)=0.36  SelLoss=7.206  CtrlLoss=7.009\n",
      "[0350] LM=7.024  Router=0.026  Ent=3.466  PPL=1053.93  Tokens=0.36M  Overlap=0.00  Coverage=12.39%  Skew(CV)=0.39  SelLoss=7.020  CtrlLoss=6.992\n",
      "[0400] LM=7.329  Router=-0.005  Ent=3.466  PPL=1017.47  Tokens=0.41M  Overlap=0.00  Coverage=13.67%  Skew(CV)=0.41  SelLoss=7.328  CtrlLoss=6.604\n",
      "[0450] LM=7.206  Router=-0.007  Ent=3.466  PPL=987.00  Tokens=0.46M  Overlap=0.00  Coverage=15.03%  Skew(CV)=0.42  SelLoss=7.212  CtrlLoss=6.579\n",
      "[0500] LM=7.183  Router=-0.034  Ent=3.465  PPL=961.30  Tokens=0.51M  Overlap=0.00  Coverage=16.18%  Skew(CV)=0.45  SelLoss=7.187  CtrlLoss=6.734\n",
      "[0550] LM=6.998  Router=0.008  Ent=3.465  PPL=938.62  Tokens=0.56M  Overlap=0.00  Coverage=17.41%  Skew(CV)=0.47  SelLoss=6.990  CtrlLoss=6.792\n",
      "[0600] LM=6.840  Router=-0.012  Ent=3.465  PPL=919.32  Tokens=0.61M  Overlap=0.00  Coverage=18.47%  Skew(CV)=0.48  SelLoss=6.848  CtrlLoss=6.673\n",
      "[0650] LM=7.074  Router=-0.068  Ent=3.465  PPL=900.65  Tokens=0.67M  Overlap=0.00  Coverage=19.51%  Skew(CV)=0.50  SelLoss=7.066  CtrlLoss=6.538\n",
      "[0700] LM=6.904  Router=-0.009  Ent=3.465  PPL=885.31  Tokens=0.72M  Overlap=0.00  Coverage=20.44%  Skew(CV)=0.51  SelLoss=6.899  CtrlLoss=6.784\n",
      "[0750] LM=7.019  Router=-0.007  Ent=3.465  PPL=872.87  Tokens=0.77M  Overlap=0.00  Coverage=21.34%  Skew(CV)=0.52  SelLoss=7.026  CtrlLoss=6.312\n",
      "[0800] LM=6.873  Router=-0.030  Ent=3.465  PPL=854.52  Tokens=0.82M  Overlap=0.00  Coverage=22.36%  Skew(CV)=0.52  SelLoss=6.866  CtrlLoss=6.237\n",
      "[0850] LM=6.726  Router=0.000  Ent=3.464  PPL=848.64  Tokens=0.87M  Overlap=0.00  Coverage=23.14%  Skew(CV)=0.53  SelLoss=6.715  CtrlLoss=6.773\n",
      "[0900] LM=6.709  Router=0.036  Ent=3.463  PPL=833.59  Tokens=0.92M  Overlap=0.00  Coverage=23.87%  Skew(CV)=0.54  SelLoss=6.693  CtrlLoss=6.345\n",
      "[0950] LM=6.856  Router=0.101  Ent=3.464  PPL=820.43  Tokens=0.97M  Overlap=0.00  Coverage=24.71%  Skew(CV)=0.55  SelLoss=6.854  CtrlLoss=6.405\n",
      "[1000] LM=6.705  Router=0.170  Ent=3.464  PPL=818.34  Tokens=1.02M  Overlap=0.00  Coverage=25.24%  Skew(CV)=0.56  SelLoss=6.708  CtrlLoss=6.175\n",
      "[1050] LM=6.628  Router=-0.041  Ent=3.464  PPL=808.26  Tokens=1.08M  Overlap=0.00  Coverage=25.80%  Skew(CV)=0.57  SelLoss=6.636  CtrlLoss=6.482\n",
      "[1100] LM=6.833  Router=-0.060  Ent=3.463  PPL=800.18  Tokens=1.13M  Overlap=0.00  Coverage=26.44%  Skew(CV)=0.58  SelLoss=6.831  CtrlLoss=6.662\n",
      "[1150] LM=6.677  Router=-0.024  Ent=3.464  PPL=788.54  Tokens=1.18M  Overlap=0.00  Coverage=26.99%  Skew(CV)=0.58  SelLoss=6.690  CtrlLoss=6.261\n",
      "[1200] LM=6.675  Router=-0.078  Ent=3.463  PPL=776.77  Tokens=1.23M  Overlap=0.00  Coverage=27.55%  Skew(CV)=0.59  SelLoss=6.685  CtrlLoss=6.365\n",
      "[1250] LM=6.523  Router=-0.303  Ent=3.462  PPL=766.91  Tokens=1.28M  Overlap=0.00  Coverage=28.12%  Skew(CV)=0.59  SelLoss=6.520  CtrlLoss=6.415\n",
      "[1300] LM=6.905  Router=-0.040  Ent=3.463  PPL=761.01  Tokens=1.33M  Overlap=0.00  Coverage=28.71%  Skew(CV)=0.60  SelLoss=6.911  CtrlLoss=6.403\n",
      "[1350] LM=6.738  Router=-0.041  Ent=3.460  PPL=750.16  Tokens=1.38M  Overlap=0.00  Coverage=29.29%  Skew(CV)=0.60  SelLoss=6.762  CtrlLoss=6.140\n",
      "[1400] LM=6.551  Router=0.012  Ent=3.460  PPL=743.48  Tokens=1.43M  Overlap=0.00  Coverage=29.87%  Skew(CV)=0.61  SelLoss=6.567  CtrlLoss=6.443\n",
      "[1450] LM=6.774  Router=-0.094  Ent=3.461  PPL=740.41  Tokens=1.48M  Overlap=0.00  Coverage=30.38%  Skew(CV)=0.61  SelLoss=6.773  CtrlLoss=6.110\n",
      "[1500] LM=6.733  Router=-0.128  Ent=3.461  PPL=733.71  Tokens=1.54M  Overlap=0.00  Coverage=30.86%  Skew(CV)=0.62  SelLoss=6.749  CtrlLoss=6.437\n",
      "[1550] LM=6.557  Router=0.294  Ent=3.459  PPL=735.16  Tokens=1.59M  Overlap=0.00  Coverage=31.28%  Skew(CV)=0.62  SelLoss=6.564  CtrlLoss=6.364\n",
      "[1600] LM=6.545  Router=-0.551  Ent=3.461  PPL=724.44  Tokens=1.64M  Overlap=0.00  Coverage=31.71%  Skew(CV)=0.62  SelLoss=6.544  CtrlLoss=6.079\n",
      "[1650] LM=6.383  Router=-0.209  Ent=3.459  PPL=718.70  Tokens=1.69M  Overlap=0.00  Coverage=32.11%  Skew(CV)=0.63  SelLoss=6.397  CtrlLoss=6.293\n",
      "[1700] LM=6.380  Router=-0.212  Ent=3.459  PPL=716.92  Tokens=1.74M  Overlap=0.00  Coverage=32.60%  Skew(CV)=0.63  SelLoss=6.374  CtrlLoss=6.409\n",
      "[1750] LM=6.419  Router=0.028  Ent=3.460  PPL=701.44  Tokens=1.79M  Overlap=0.00  Coverage=32.95%  Skew(CV)=0.63  SelLoss=6.384  CtrlLoss=6.412\n",
      "[1800] LM=6.411  Router=0.200  Ent=3.460  PPL=696.58  Tokens=1.84M  Overlap=0.00  Coverage=33.29%  Skew(CV)=0.63  SelLoss=6.407  CtrlLoss=6.060\n",
      "[1850] LM=6.307  Router=0.413  Ent=3.459  PPL=690.82  Tokens=1.89M  Overlap=0.00  Coverage=33.61%  Skew(CV)=0.63  SelLoss=6.333  CtrlLoss=6.368\n",
      "[1900] LM=6.560  Router=-0.005  Ent=3.459  PPL=685.90  Tokens=1.95M  Overlap=0.00  Coverage=33.96%  Skew(CV)=0.63  SelLoss=6.549  CtrlLoss=6.101\n",
      "[1950] LM=6.053  Router=0.161  Ent=3.461  PPL=685.49  Tokens=2.00M  Overlap=0.00  Coverage=34.34%  Skew(CV)=0.64  SelLoss=6.056  CtrlLoss=6.458\n",
      "[2000] LM=6.240  Router=0.123  Ent=3.462  PPL=678.79  Tokens=2.05M  Overlap=0.00  Coverage=34.73%  Skew(CV)=0.64  SelLoss=6.257  CtrlLoss=6.598\n"
     ]
    }
   ],
   "source": [
    "cfg = Cfg()\n",
    "\n",
    "# Tokenizer (use GPT-2 tokenizer for simplicity; set pad to eos)\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "# Build datasets\n",
    "x_tr, y_tr = build_wikitext2(tok, \"train\", cfg.seq_len)\n",
    "x_va, y_va = build_wikitext2(tok, \"validation\", cfg.seq_len)\n",
    "\n",
    "cfg.vocab = tok.vocab_size\n",
    "train_ds = TensorDatasetSimple(x_tr, y_tr)\n",
    "val_ds   = TensorDatasetSimple(x_va, y_va)\n",
    "\n",
    "# Simple random sampler for candidate pools\n",
    "def sample_pool(ds, M):\n",
    "    idxs = random.sample(range(len(ds)), M)\n",
    "    batch = [ds[i] for i in idxs]\n",
    "    return idxs, batch\n",
    "\n",
    "N = len(train_ds)\n",
    "select_counts = torch.zeros(N, dtype=torch.long)   # utilization per sample\n",
    "seen_selected = set()                               # coverage set\n",
    "last_selected = None                                # for overlap\n",
    "entropy_track = []\n",
    "tokens_seen = 0\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=cfg.val_bs, shuffle=False, drop_last=False)\n",
    "\n",
    "# Models\n",
    "lm = TinyCausalLM(vocab_size=cfg.vocab, d=Cfg.model_dim, n_heads=Cfg.n_heads,\n",
    "                    n_layers=Cfg.n_layers, ff=Cfg.ff_dim, dropout=Cfg.dropout,\n",
    "                    seq_len=Cfg.seq_len).to(cfg.device)\n",
    "router = AttentionRouter(d_in=Cfg.model_dim, d_attn=128).to(cfg.device)\n",
    "\n",
    "# Optimizer (single opt for both)\n",
    "opt = torch.optim.AdamW(list(lm.parameters()) + list(router.parameters()),\n",
    "                        lr=cfg.lr, weight_decay=cfg.wd, betas=(0.9, 0.95))\n",
    "\n",
    "print(f\"Device={cfg.device} | WT2 samples: train={len(train_ds)} val={len(val_ds)} | pool M={cfg.pool_M}, k={cfg.sel_k}\")\n",
    "\n",
    "for step in range(1, cfg.batch_steps + 1):\n",
    "    pool_indices, pool = sample_pool(train_ds, cfg.pool_M)\n",
    "    stats = train_step(cfg, lm, router, pool, pool_indices, opt)\n",
    "\n",
    "    # tokens processed\n",
    "    tokens_seen += cfg.sel_k * cfg.seq_len\n",
    "    entropy_track.append(stats[\"entropy\"])\n",
    "\n",
    "    # utilization + coverage\n",
    "    for gi in stats[\"selected_indices\"]:\n",
    "        select_counts[gi] += 1\n",
    "        seen_selected.add(gi)\n",
    "\n",
    "    # overlap (Jaccard) with previous step\n",
    "    if last_selected is None:\n",
    "        overlap = float(\"nan\")\n",
    "    else:\n",
    "        a = set(last_selected); b = set(stats[\"selected_indices\"])\n",
    "        inter = len(a & b); uni = len(a | b)\n",
    "        overlap = inter / uni if uni > 0 else float(\"nan\")\n",
    "    last_selected = list(stats[\"selected_indices\"])\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        ppl = lm.ppl(val_loader)\n",
    "        coverage = len(seen_selected) / N\n",
    "        # simple skew: coefficient of variation of counts over seen_selected\n",
    "        seen_counts = select_counts[list(seen_selected)].float() if seen_selected else torch.tensor([0.0])\n",
    "        if len(seen_counts) > 1:\n",
    "            cv = (seen_counts.std() / (seen_counts.mean() + 1e-8)).item()\n",
    "        else:\n",
    "            cv = float(\"nan\")\n",
    "\n",
    "        print(\n",
    "            f\"[{step:04d}] \"\n",
    "            f\"LM={stats['loss_lm']:.3f}  \"\n",
    "            f\"Router={stats['loss_router']:.3f}  \"\n",
    "            f\"Ent={stats['entropy']:.3f}  \"\n",
    "            f\"PPL={ppl:.2f}  \"\n",
    "            f\"Tokens={tokens_seen/1e6:.2f}M  \"\n",
    "            f\"Overlap={overlap:.2f}  \"\n",
    "            f\"Coverage={coverage:.2%}  \"\n",
    "            f\"Skew(CV)={cv:.2f}  \"\n",
    "            f\"SelLoss={stats['sel_mean_loss']:.3f}  \"\n",
    "            f\"CtrlLoss={stats['ctrl_mean_loss']:.3f}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curriculum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
