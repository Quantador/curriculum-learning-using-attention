{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b22cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, random\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cdde79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0); torch.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7416f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK = 256          # seq len\n",
    "BATCH = 16           # final selected batch size (k)\n",
    "POOL_MULT = 2        # candidate pool size multiplier -> M = POOL_MULT * BATCH\n",
    "EPOCHS = 3 * POOL_MULT  # number of epochs\n",
    "LR_LM = 3e-4\n",
    "LR_ROUTER = 1e-3\n",
    "TEMP = 1.0\n",
    "LAMBDA_ENT = 1e-2    # entropy reg for router\n",
    "LAMBDA_ROUTER = 0.1  # weight for router loss\n",
    "\n",
    "POOL = POOL_MULT * BATCH\n",
    "\n",
    "tok = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04cf319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(split):\n",
    "    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=split)\n",
    "    text = tok.eos_token.join(ds[\"text\"])\n",
    "    ids = tok(text, add_special_tokens=False)[\"input_ids\"]\n",
    "    L = (len(ids) // (BLOCK + 1)) * (BLOCK + 1)\n",
    "    ids = ids[:L]\n",
    "    chunks = [ids[i:i+BLOCK+1] for i in range(0, L, BLOCK+1)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884244c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, chunks):\n",
    "        self.x = [torch.tensor(c[:-1], dtype=torch.long) for c in chunks]\n",
    "        self.y = [torch.tensor(c[1:],  dtype=torch.long) for c in chunks]\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d947e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/wikitext.py'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/.huggingface.yaml'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/dataset_infos.json'\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2428601 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/wikitext.py'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/.huggingface.yaml'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/dataset_infos.json'\n"
     ]
    }
   ],
   "source": [
    "train_chunks = make_chunks(\"train\")\n",
    "val_chunks   = make_chunks(\"validation\")\n",
    "train_ds = LMDataset(train_chunks)\n",
    "val_ds   = LMDataset(val_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ad85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index_loader(ds_len, pool_size):\n",
    "    order = list(range(ds_len))\n",
    "    random.shuffle(order)\n",
    "    for i in range(0, ds_len, pool_size):\n",
    "        yield order[i:i+pool_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d05b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab, d_model=256, n_layers=4, n_heads=8, d_ff=1024, block=BLOCK):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.tok_emb = nn.Embedding(vocab, d_model)\n",
    "        self.pos_emb = nn.Embedding(block, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.tr = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab, bias=False)\n",
    "        # tie weights\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "        \n",
    "    def _causal_mask(self, L):\n",
    "        # [L, L] upper-triangular mask True where we want to mask (future tokens)\n",
    "        m = torch.ones(L, L, dtype=torch.bool, device=self.lm_head.weight.device).triu(1)\n",
    "        return m\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape\n",
    "        pos = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        h = self.tok_emb(x) + self.pos_emb(pos)\n",
    "        mask = self._causal_mask(L)\n",
    "        h = self.tr(h, mask=mask)\n",
    "        return self.lm_head(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e423950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRouter(nn.Module):\n",
    "    def __init__(self, d_model=256, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, feats):            # feats: [M, d_model]\n",
    "        return self.net(feats).squeeze(-1)  # [M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae84a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionRouter(nn.Module):\n",
    "    def __init__(self, d_model=256, d_k=64):\n",
    "        super().__init__()\n",
    "        self.K = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.q = nn.Parameter(torch.randn(d_k) / math.sqrt(d_k))\n",
    "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        \n",
    "    def forward(self, feats):\n",
    "        K = self.K(feats)\n",
    "        scores = (K @ self.q) / math.sqrt(K.size(-1))\n",
    "        scores = scores * torch.abs(self.scale)\n",
    "        return scores  # [M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0cdc4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionRouter(nn.Module):\n",
    "    def __init__(self, d_model=256, d_k=64, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.n = n_heads\n",
    "        self.K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.q = nn.Parameter(torch.randn(n_heads, d_k) / math.sqrt(d_k))\n",
    "        self.out = nn.Linear(n_heads, 1, bias=False)  # combine head scores\n",
    "\n",
    "    def forward(self, feats):                      # feats: [M, d_model]\n",
    "        M = feats.size(0)\n",
    "        K = self.K(feats).view(M, self.n, -1)      # [M, H, d_k]\n",
    "        # per-head scores: [M, H]\n",
    "        scores_h = (K * self.q).sum(dim=-1) / math.sqrt(K.size(-1))\n",
    "        # combine heads linearly â†’ [M]\n",
    "        scores = self.out(scores_h).squeeze(-1)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f239360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_ds, loss_fn):\n",
    "    model.eval()\n",
    "    loader = DataLoader(val_ds, batch_size=BATCH, shuffle=False)\n",
    "    losses = []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "        losses.append(loss.item())\n",
    "    m = sum(losses)/len(losses)\n",
    "    ppl = math.exp(min(20.0, m))\n",
    "    return m, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63ecf3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce():\n",
    "    \n",
    "    model = TinyGPT(vocab_size).to(device)\n",
    "    # router = MLPRouter(d_model=model.tok_emb.embedding_dim).to(device)\n",
    "    router = MultiHeadAttentionRouter(d_model=model.tok_emb.embedding_dim, d_k=64, n_heads=2).to(device)\n",
    "    opt_lm = torch.optim.AdamW(model.parameters(), lr=LR_LM)\n",
    "    opt_router = torch.optim.AdamW(router.parameters(), lr=LR_ROUTER)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train(); router.train()\n",
    "        idx_loader = make_index_loader(len(train_ds), POOL)\n",
    "        step = 0\n",
    "        for pool_indices in idx_loader:\n",
    "            if len(pool_indices) < BATCH:   # small tail at end of epoch\n",
    "                continue\n",
    "            step += 1\n",
    "\n",
    "            # 1) Load candidate pool to CPU tensors\n",
    "            xs, ys = zip(*[train_ds[i] for i in pool_indices])  # tuples of [L]\n",
    "            X = torch.stack(xs, 0).to(device)  # [M, L]\n",
    "            Y = torch.stack(ys, 0).to(device)  # [M, L]\n",
    "            M = X.size(0)\n",
    "\n",
    "            # 2) Cheap features: mean token embedding per sample (no grad)\n",
    "            with torch.no_grad():\n",
    "                emb = model.tok_emb(X)             # [M, L, d]\n",
    "                feats = emb.mean(dim=1)            # [M, d]\n",
    "\n",
    "            # 3) Router scoring + softmax probs\n",
    "            scores = router(feats)                 # [M]\n",
    "            probs = torch.softmax(scores / TEMP, dim=0)  # [M]\n",
    "\n",
    "            # 4) Hard select top-k\n",
    "            k = BATCH\n",
    "            topk_idx = torch.topk(probs, k=k, dim=0).indices  # [k]\n",
    "            sel_mask = torch.zeros(M, device=device)\n",
    "            sel_mask[topk_idx] = 1.0\n",
    "\n",
    "            # 5) Build selected batch\n",
    "            X_sel = X[topk_idx]   # [k, L]\n",
    "            Y_sel = Y[topk_idx]   # [k, L]\n",
    "\n",
    "            # 6) LM forward on selected only\n",
    "            logits = model(X_sel)                                 # [k, L, V]\n",
    "            loss_lm = loss_fn(logits.reshape(-1, logits.size(-1)), Y_sel.reshape(-1))\n",
    "\n",
    "            # 7) Router loss (REINFORCE-style on selected only) + entropy regularizer\n",
    "            #    reward ~ higher when LM loss is high -> encourages selecting informative samples\n",
    "            with torch.no_grad():\n",
    "                per_sample_loss = nn.functional.cross_entropy(\n",
    "                    logits.detach().reshape(-1, logits.size(-1)),\n",
    "                    Y_sel.reshape(-1),\n",
    "                    reduction='none'\n",
    "                ).reshape(k, -1).mean(dim=1)  # [k]\n",
    "                baseline = per_sample_loss.mean()\n",
    "\n",
    "            # map selected indices back to their probs\n",
    "            sel_probs = probs[topk_idx].clamp_min(1e-12)  # [k]\n",
    "            reinforce = - ((per_sample_loss - baseline) * torch.log(sel_probs)).mean()\n",
    "\n",
    "            # entropy over full pool\n",
    "            ent = (probs * torch.log(probs.clamp_min(1e-12))).sum()\n",
    "\n",
    "            loss_router = reinforce + LAMBDA_ENT * ent\n",
    "\n",
    "            # 8) Combined step (separate opts for clarity)\n",
    "            opt_lm.zero_grad(set_to_none=True)\n",
    "            opt_router.zero_grad(set_to_none=True)\n",
    "            (loss_lm + LAMBDA_ROUTER * loss_router).backward()\n",
    "            opt_lm.step()\n",
    "            opt_router.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"epoch {epoch} step {step}: \"\n",
    "                    f\"LM={loss_lm.item():.4f}  \"\n",
    "                    f\"Router(reinf)={reinforce.item():.4f}  \"\n",
    "                    f\"H(p)={(-ent).item():.3f}\")\n",
    "\n",
    "        val_loss, val_ppl = evaluate(model, val_ds, loss_fn)\n",
    "        print(f\"==> epoch {epoch}: val_loss={val_loss:.4f}  val_ppl={val_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d100c3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 100: LM=28.5971  Router(reinf)=-0.0226  H(p)=3.457\n",
      "epoch 1 step 200: LM=21.2994  Router(reinf)=-0.2674  H(p)=3.295\n",
      "==> epoch 1: val_loss=14.3398  val_ppl=1689193.02\n",
      "epoch 2 step 100: LM=13.4049  Router(reinf)=-0.2982  H(p)=2.807\n",
      "epoch 2 step 200: LM=12.2962  Router(reinf)=-1.1576  H(p)=0.721\n",
      "==> epoch 2: val_loss=9.7780  val_ppl=17640.66\n",
      "epoch 3 step 100: LM=10.1138  Router(reinf)=-0.9745  H(p)=0.802\n",
      "epoch 3 step 200: LM=9.5649  Router(reinf)=-1.0903  H(p)=0.680\n",
      "==> epoch 3: val_loss=8.3823  val_ppl=4369.20\n",
      "epoch 4 step 100: LM=8.7358  Router(reinf)=-0.6630  H(p)=0.796\n",
      "epoch 4 step 200: LM=8.5281  Router(reinf)=-1.6028  H(p)=0.713\n",
      "==> epoch 4: val_loss=7.8229  val_ppl=2497.18\n",
      "epoch 5 step 100: LM=8.1532  Router(reinf)=-2.6274  H(p)=0.000\n",
      "epoch 5 step 200: LM=8.1537  Router(reinf)=-1.0042  H(p)=0.290\n",
      "==> epoch 5: val_loss=7.5658  val_ppl=1930.99\n",
      "epoch 6 step 100: LM=7.9292  Router(reinf)=-0.5877  H(p)=0.002\n",
      "epoch 6 step 200: LM=7.7998  Router(reinf)=-2.2136  H(p)=0.120\n",
      "==> epoch 6: val_loss=7.3410  val_ppl=1542.31\n"
     ]
    }
   ],
   "source": [
    "train_reinforce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ST():\n",
    "    \n",
    "    model = TinyGPT(vocab_size).to(device)\n",
    "    # router = MLPRouter(d_model=model.tok_emb.embedding_dim).to(device)\n",
    "    router = AttentionRouter(d_model=model.tok_emb.embedding_dim, d_k=64).to(device)\n",
    "    #router = MultiHeadAttentionRouter(d_model=model.tok_emb.embedding_dim, d_k=64, n_heads=2).to(device)\n",
    "    opt_lm = torch.optim.AdamW(model.parameters(), lr=LR_LM)\n",
    "    opt_router = torch.optim.AdamW(router.parameters(), lr=LR_ROUTER)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train(); router.train()\n",
    "        idx_loader = make_index_loader(len(train_ds), POOL)\n",
    "        step = 0\n",
    "        for pool_indices in idx_loader:\n",
    "            if len(pool_indices) < BATCH:   # small tail at end of epoch\n",
    "                continue\n",
    "            step += 1\n",
    "\n",
    "            # 1) Load candidate pool to CPU tensors\n",
    "            xs, ys = zip(*[train_ds[i] for i in pool_indices])  # tuples of [L]\n",
    "            X = torch.stack(xs, 0).to(device)  # [M, L]\n",
    "            Y = torch.stack(ys, 0).to(device)  # [M, L]\n",
    "            M = X.size(0)\n",
    "\n",
    "            # 2) Cheap features: mean token embedding per sample (no grad)\n",
    "            with torch.no_grad():\n",
    "                emb = model.tok_emb(X)             # [M, L, d]\n",
    "                feats = emb.mean(dim=1)            # [M, d]\n",
    "\n",
    "            # 3) Router scoring + softmax probs\n",
    "            scores = router(feats)                 # [M]\n",
    "            probs = torch.softmax(scores / TEMP, dim=0)  # [M]\n",
    "            logp   = torch.log(probs.clamp_min(1e-12))\n",
    "\n",
    "\n",
    "            # 4) Hard select top-k\n",
    "            k = BATCH\n",
    "            topk_idx = torch.topk(probs, k=k, dim=0).indices  # [k]\n",
    "            sel_mask_hard = torch.zeros(M, device=device)\n",
    "            sel_mask_hard[topk_idx] = 1.0\n",
    "            \n",
    "            sel_mask_st = (sel_mask_hard - probs).detach() + probs    # [M]\n",
    "\n",
    "            # 5) Build selected batch\n",
    "            X_sel = X[topk_idx]   # [k, L]\n",
    "            Y_sel = Y[topk_idx]   # [k, L]\n",
    "\n",
    "            # 6) LM forward on selected only\n",
    "            logits = model(X_sel)                                 # [k, L, V]\n",
    "            loss_lm = loss_fn(logits.reshape(-1, logits.size(-1)), Y_sel.reshape(-1))\n",
    "\n",
    "            # 7) Router loss (ST surrogate) + entropy reg\n",
    "            ce_align = -(sel_mask_st * logp).sum() / k         # cross-entropy to one-hot (averaged)\n",
    "            ent = (probs * logp).sum()                         # negative entropy (to penalize collapse)\n",
    "\n",
    "            loss_router = ce_align + LAMBDA_ENT * ent\n",
    "\n",
    "            # 8) Combined step (separate opts for clarity)\n",
    "            opt_lm.zero_grad(set_to_none=True)\n",
    "            opt_router.zero_grad(set_to_none=True)\n",
    "            (loss_lm + LAMBDA_ROUTER * loss_router).backward()\n",
    "            opt_lm.step()\n",
    "            opt_router.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"epoch {epoch} step {step}: \"\n",
    "                    f\"LM={loss_lm.item():.4f}  \"\n",
    "                    f\"Router(ce_align)={ce_align.item():.4f}  \"\n",
    "                    f\"H(p)={(-ent).item():.3f}\")\n",
    "\n",
    "        val_loss, val_ppl = evaluate(model, val_ds, loss_fn)\n",
    "        print(f\"==> epoch {epoch}: val_loss={val_loss:.4f}  val_ppl={val_ppl:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curriculum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
