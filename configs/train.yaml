output_dir: outputs/baseline-wt2
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
gradient_accumulation_steps: 4
learning_rate: 3.0e-4
weight_decay: 0.1
warmup_steps: 2000
num_train_epochs: 5
lr_scheduler_type: cosine
logging_steps: 50
eval_steps: 500
save_steps: 1000
fp16: true
