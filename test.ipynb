{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b22cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, random\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cdde79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0); torch.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7416f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK = 256      # sequence length (tokens)\n",
    "BATCH = 16       # batch size\n",
    "EPOCHS = 3\n",
    "LR = 3e-4\n",
    "\n",
    "tok = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04cf319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(split):\n",
    "    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=split)\n",
    "    text = tok.eos_token.join(ds[\"text\"])\n",
    "    ids = tok(text, add_special_tokens=False)[\"input_ids\"]\n",
    "    # we want input length BLOCK and target length BLOCK, so make windows of BLOCK+1 then split\n",
    "    L = (len(ids) // (BLOCK + 1)) * (BLOCK + 1)\n",
    "    ids = ids[:L]\n",
    "    chunks = [ids[i:i+BLOCK+1] for i in range(0, L, BLOCK+1)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "884244c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, chunks):\n",
    "        self.x = [torch.tensor(c[:-1], dtype=torch.long) for c in chunks]\n",
    "        self.y = [torch.tensor(c[1:],  dtype=torch.long) for c in chunks]\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31d947e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/wikitext.py'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/.huggingface.yaml'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/dataset_infos.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/wikitext.py'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/.huggingface.yaml'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/dataset_infos.json'\n"
     ]
    }
   ],
   "source": [
    "train_chunks = make_chunks(\"train\")\n",
    "val_chunks   = make_chunks(\"validation\")\n",
    "train_loader = DataLoader(LMDataset(train_chunks), batch_size=BATCH, shuffle=True)\n",
    "val_loader   = DataLoader(LMDataset(val_chunks),   batch_size=BATCH, shuffle=False)\n",
    "\n",
    "vocab_size = len(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d05b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab, d_model=256, n_layers=4, n_heads=8, d_ff=1024, block=BLOCK):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.tok_emb = nn.Embedding(vocab, d_model)\n",
    "        self.pos_emb = nn.Embedding(block, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.tr = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab, bias=False)\n",
    "        # tie weights\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "        \n",
    "    def _causal_mask(self, L):\n",
    "        # [L, L] upper-triangular mask True where we want to mask (future tokens)\n",
    "        m = torch.ones(L, L, dtype=torch.bool, device=self.lm_head.weight.device).triu(1)\n",
    "        return m\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape\n",
    "        pos = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        h = self.tok_emb(x) + self.pos_emb(pos)\n",
    "        mask = self._causal_mask(L)\n",
    "        h = self.tr(h, mask=mask)\n",
    "        return self.lm_head(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "793b3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyGPT(vocab_size).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f239360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)                      # [B, L, V]\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        losses.append(loss.item())\n",
    "    m = sum(losses)/len(losses)\n",
    "    ppl = math.exp(min(20.0, m))\n",
    "    return m, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63ecf3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 100: train_loss=36.2972\n",
      "epoch 1 step 200: train_loss=23.2266\n",
      "epoch 1 step 300: train_loss=17.6469\n",
      "epoch 1 step 400: train_loss=14.3102\n",
      "epoch 1 step 500: train_loss=12.2759\n",
      "==> epoch 1: val_loss=9.8756  val_ppl=19450.40\n",
      "epoch 2 step 100: train_loss=10.1666\n",
      "epoch 2 step 200: train_loss=9.5792\n",
      "epoch 2 step 300: train_loss=9.0992\n",
      "epoch 2 step 400: train_loss=8.7476\n",
      "epoch 2 step 500: train_loss=8.4784\n",
      "==> epoch 2: val_loss=7.8337  val_ppl=2524.35\n",
      "epoch 3 step 100: train_loss=8.0183\n",
      "epoch 3 step 200: train_loss=7.9145\n",
      "epoch 3 step 300: train_loss=7.7985\n",
      "epoch 3 step 400: train_loss=7.7028\n",
      "epoch 3 step 500: train_loss=7.6226\n",
      "==> epoch 3: val_loss=7.3498  val_ppl=1555.94\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for i, (x, y) in enumerate(train_loader, start=1):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)                                # [B, L, V]\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"epoch {epoch} step {i}: train_loss={running/100:.4f}\")\n",
    "            running = 0.0\n",
    "    val_loss, val_ppl = evaluate()\n",
    "    print(f\"==> epoch {epoch}: val_loss={val_loss:.4f}  val_ppl={val_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f98cb65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curriculum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
