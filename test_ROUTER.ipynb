{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4b22cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, random\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cdde79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0); torch.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7416f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK = 256          # seq len\n",
    "BATCH = 16           # final selected batch size (k)\n",
    "POOL_MULT = 4        # candidate pool size multiplier -> M = POOL_MULT * BATCH\n",
    "EPOCHS = 3*5\n",
    "LR_LM = 3e-4\n",
    "LR_ROUTER = 1e-3\n",
    "TEMP = 1.0\n",
    "LAMBDA_ENT = 1e-3    # entropy reg for router\n",
    "LAMBDA_ROUTER = 0.1  # weight for router loss\n",
    "\n",
    "tok = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04cf319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(split):\n",
    "    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=split)\n",
    "    text = tok.eos_token.join(ds[\"text\"])\n",
    "    ids = tok(text, add_special_tokens=False)[\"input_ids\"]\n",
    "    L = (len(ids) // (BLOCK + 1)) * (BLOCK + 1)\n",
    "    ids = ids[:L]\n",
    "    chunks = [ids[i:i+BLOCK+1] for i in range(0, L, BLOCK+1)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "884244c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, chunks):\n",
    "        self.x = [torch.tensor(c[:-1], dtype=torch.long) for c in chunks]\n",
    "        self.y = [torch.tensor(c[1:],  dtype=torch.long) for c in chunks]\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31d947e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/wikitext.py'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/.huggingface.yaml'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/dataset_infos.json'\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2428601 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/wikitext.py'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/.huggingface.yaml'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mloscratch/hf_cache/hub/datasets--wikitext/.no_exist/b08601e04326c79dfdd32d625aee71d232d685c3/dataset_infos.json'\n"
     ]
    }
   ],
   "source": [
    "train_chunks = make_chunks(\"train\")\n",
    "val_chunks   = make_chunks(\"validation\")\n",
    "train_ds = LMDataset(train_chunks)\n",
    "val_ds   = LMDataset(val_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3ad85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index_loader(ds_len, pool_size):\n",
    "    order = list(range(ds_len))\n",
    "    random.shuffle(order)\n",
    "    for i in range(0, ds_len, pool_size):\n",
    "        yield order[i:i+pool_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f23b9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d05b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab, d_model=256, n_layers=4, n_heads=8, d_ff=1024, block=BLOCK):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.tok_emb = nn.Embedding(vocab, d_model)\n",
    "        self.pos_emb = nn.Embedding(block, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.tr = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab, bias=False)\n",
    "        # tie weights\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "        \n",
    "    def _causal_mask(self, L):\n",
    "        # [L, L] upper-triangular mask True where we want to mask (future tokens)\n",
    "        m = torch.ones(L, L, dtype=torch.bool, device=self.lm_head.weight.device).triu(1)\n",
    "        return m\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape\n",
    "        pos = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        h = self.tok_emb(x) + self.pos_emb(pos)\n",
    "        mask = self._causal_mask(L)\n",
    "        h = self.tr(h, mask=mask)\n",
    "        return self.lm_head(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e423950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRouter(nn.Module):\n",
    "    def __init__(self, d_model=256, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, feats):            # feats: [M, d_model]\n",
    "        return self.net(feats).squeeze(-1)  # [M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0cdc4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionRouter(nn.Module):\n",
    "    def __init__(self, d_model=256, d_k=64, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.n = n_heads\n",
    "        self.K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.q = nn.Parameter(torch.randn(n_heads, d_k) / math.sqrt(d_k))\n",
    "        self.out = nn.Linear(n_heads, 1, bias=False)  # combine head scores\n",
    "\n",
    "    def forward(self, feats):                      # feats: [M, d_model]\n",
    "        M = feats.size(0)\n",
    "        K = self.K(feats).view(M, self.n, -1)      # [M, H, d_k]\n",
    "        # per-head scores: [M, H]\n",
    "        scores_h = (K * self.q).sum(dim=-1) / math.sqrt(K.size(-1))\n",
    "        # combine heads linearly â†’ [M]\n",
    "        scores = self.out(scores_h).squeeze(-1)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "793b3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyGPT(vocab_size).to(device)\n",
    "# router = MLPRouter(d_model=model.tok_emb.embedding_dim).to(device)\n",
    "router = MultiHeadAttentionRouter(d_model=model.tok_emb.embedding_dim, d_k=64, n_heads=2).to(device)\n",
    "opt_lm = torch.optim.AdamW(model.parameters(), lr=LR_LM)\n",
    "opt_router = torch.optim.AdamW(router.parameters(), lr=LR_ROUTER)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f239360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    loader = DataLoader(val_ds, batch_size=BATCH, shuffle=False)\n",
    "    losses = []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "        losses.append(loss.item())\n",
    "    m = sum(losses)/len(losses)\n",
    "    ppl = math.exp(min(20.0, m))\n",
    "    return m, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63ecf3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "POOL = POOL_MULT * BATCH\n",
    "\n",
    "\n",
    "def train_reinforce():\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train(); router.train()\n",
    "        idx_loader = make_index_loader(len(train_ds), POOL)\n",
    "        step = 0\n",
    "        for pool_indices in idx_loader:\n",
    "            if len(pool_indices) < BATCH:   # small tail at end of epoch\n",
    "                continue\n",
    "            step += 1\n",
    "\n",
    "            # 1) Load candidate pool to CPU tensors\n",
    "            xs, ys = zip(*[train_ds[i] for i in pool_indices])  # tuples of [L]\n",
    "            X = torch.stack(xs, 0).to(device)  # [M, L]\n",
    "            Y = torch.stack(ys, 0).to(device)  # [M, L]\n",
    "            M = X.size(0)\n",
    "\n",
    "            # 2) Cheap features: mean token embedding per sample (no grad)\n",
    "            with torch.no_grad():\n",
    "                emb = model.tok_emb(X)             # [M, L, d]\n",
    "                feats = emb.mean(dim=1)            # [M, d]\n",
    "\n",
    "            # 3) Router scoring + softmax probs\n",
    "            scores = router(feats)                 # [M]\n",
    "            probs = torch.softmax(scores / TEMP, dim=0)  # [M]\n",
    "\n",
    "            # 4) Hard select top-k\n",
    "            k = BATCH\n",
    "            topk_idx = torch.topk(probs, k=k, dim=0).indices  # [k]\n",
    "            sel_mask = torch.zeros(M, device=device)\n",
    "            sel_mask[topk_idx] = 1.0\n",
    "\n",
    "            # 5) Build selected batch\n",
    "            X_sel = X[topk_idx]   # [k, L]\n",
    "            Y_sel = Y[topk_idx]   # [k, L]\n",
    "\n",
    "            # 6) LM forward on selected only\n",
    "            logits = model(X_sel)                                 # [k, L, V]\n",
    "            loss_lm = loss_fn(logits.reshape(-1, logits.size(-1)), Y_sel.reshape(-1))\n",
    "\n",
    "            # 7) Router loss (REINFORCE-style on selected only) + entropy regularizer\n",
    "            #    reward ~ higher when LM loss is high -> encourages selecting informative samples\n",
    "            with torch.no_grad():\n",
    "                per_sample_loss = nn.functional.cross_entropy(\n",
    "                    logits.detach().reshape(-1, logits.size(-1)),\n",
    "                    Y_sel.reshape(-1),\n",
    "                    reduction='none'\n",
    "                ).reshape(k, -1).mean(dim=1)  # [k]\n",
    "                baseline = per_sample_loss.mean()\n",
    "\n",
    "            # map selected indices back to their probs\n",
    "            sel_probs = probs[topk_idx].clamp_min(1e-12)  # [k]\n",
    "            reinforce = - ((per_sample_loss - baseline) * torch.log(sel_probs)).mean()\n",
    "\n",
    "            # entropy over full pool\n",
    "            ent = (probs * torch.log(probs.clamp_min(1e-12))).sum()\n",
    "\n",
    "            loss_router = reinforce + LAMBDA_ENT * ent\n",
    "\n",
    "            # 8) Combined step (separate opts for clarity)\n",
    "            opt_lm.zero_grad(set_to_none=True)\n",
    "            opt_router.zero_grad(set_to_none=True)\n",
    "            (loss_lm + LAMBDA_ROUTER * loss_router).backward()\n",
    "            opt_lm.step()\n",
    "            opt_router.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"epoch {epoch} step {step}: \"\n",
    "                    f\"LM={loss_lm.item():.4f}  \"\n",
    "                    f\"Router(reinf)={reinforce.item():.4f}  \"\n",
    "                    f\"H(p)={(-ent).item():.3f}\")\n",
    "\n",
    "        val_loss, val_ppl = evaluate()\n",
    "        print(f\"==> epoch {epoch}: val_loss={val_loss:.4f}  val_ppl={val_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72b20ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ST():\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train(); router.train()\n",
    "        idx_loader = make_index_loader(len(train_ds), POOL)\n",
    "        step = 0\n",
    "        for pool_indices in idx_loader:\n",
    "            if len(pool_indices) < BATCH:   # small tail at end of epoch\n",
    "                continue\n",
    "            step += 1\n",
    "\n",
    "            # 1) Load candidate pool to CPU tensors\n",
    "            xs, ys = zip(*[train_ds[i] for i in pool_indices])  # tuples of [L]\n",
    "            X = torch.stack(xs, 0).to(device)  # [M, L]\n",
    "            Y = torch.stack(ys, 0).to(device)  # [M, L]\n",
    "            M = X.size(0)\n",
    "\n",
    "            # 2) Cheap features: mean token embedding per sample (no grad)\n",
    "            with torch.no_grad():\n",
    "                emb = model.tok_emb(X)             # [M, L, d]\n",
    "                feats = emb.mean(dim=1)            # [M, d]\n",
    "\n",
    "            # 3) Router scoring + softmax probs\n",
    "            scores = router(feats)                 # [M]\n",
    "            probs = torch.softmax(scores / TEMP, dim=0)  # [M]\n",
    "            logp   = torch.log(probs.clamp_min(1e-12))\n",
    "\n",
    "\n",
    "            # 4) Hard select top-k\n",
    "            k = BATCH\n",
    "            topk_idx = torch.topk(probs, k=k, dim=0).indices  # [k]\n",
    "            sel_mask_hard = torch.zeros(M, device=device)\n",
    "            sel_mask_hard[topk_idx] = 1.0\n",
    "            \n",
    "            sel_mask_st = (sel_mask_hard - probs).detach() + probs    # [M]\n",
    "\n",
    "            # 5) Build selected batch\n",
    "            X_sel = X[topk_idx]   # [k, L]\n",
    "            Y_sel = Y[topk_idx]   # [k, L]\n",
    "\n",
    "            # 6) LM forward on selected only\n",
    "            logits = model(X_sel)                                 # [k, L, V]\n",
    "            loss_lm = loss_fn(logits.reshape(-1, logits.size(-1)), Y_sel.reshape(-1))\n",
    "\n",
    "            # 7) Router loss (ST surrogate) + entropy reg\n",
    "            ce_align = -(sel_mask_st * logp).sum() / k         # cross-entropy to one-hot (averaged)\n",
    "            ent = (probs * logp).sum()                         # negative entropy (to penalize collapse)\n",
    "\n",
    "            loss_router = ce_align - LAMBDA_ENT * ent\n",
    "\n",
    "            # 8) Combined step (separate opts for clarity)\n",
    "            opt_lm.zero_grad(set_to_none=True)\n",
    "            opt_router.zero_grad(set_to_none=True)\n",
    "            (loss_lm + LAMBDA_ROUTER * loss_router).backward()\n",
    "            opt_lm.step()\n",
    "            opt_router.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"epoch {epoch} step {step}: \"\n",
    "                    f\"LM={loss_lm.item():.4f}  \"\n",
    "                    f\"Router(ce_align)={ce_align.item():.4f}  \"\n",
    "                    f\"H(p)={(-ent).item():.3f}\")\n",
    "\n",
    "        val_loss, val_ppl = evaluate()\n",
    "        print(f\"==> epoch {epoch}: val_loss={val_loss:.4f}  val_ppl={val_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b73006b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 100: LM=27.0264  Router(ce_align)=3.5310  H(p)=3.974\n",
      "==> epoch 1: val_loss=21.5677  val_ppl=485165195.41\n",
      "epoch 2 step 100: LM=17.6802  Router(ce_align)=3.4887  H(p)=3.094\n",
      "==> epoch 2: val_loss=14.8690  val_ppl=2867515.82\n",
      "epoch 3 step 100: LM=12.8772  Router(ce_align)=3.2656  H(p)=3.220\n",
      "==> epoch 3: val_loss=11.6847  val_ppl=118742.13\n",
      "epoch 4 step 100: LM=10.9232  Router(ce_align)=3.2513  H(p)=3.615\n",
      "==> epoch 4: val_loss=10.0672  val_ppl=23556.70\n",
      "epoch 5 step 100: LM=9.8156  Router(ce_align)=3.2058  H(p)=3.336\n",
      "==> epoch 5: val_loss=9.1602  val_ppl=9510.83\n",
      "epoch 6 step 100: LM=9.3147  Router(ce_align)=3.4472  H(p)=2.865\n",
      "==> epoch 6: val_loss=8.6222  val_ppl=5553.87\n",
      "epoch 7 step 100: LM=8.3848  Router(ce_align)=3.1391  H(p)=3.096\n",
      "==> epoch 7: val_loss=8.2281  val_ppl=3744.86\n",
      "epoch 8 step 100: LM=8.2093  Router(ce_align)=3.1370  H(p)=3.260\n",
      "==> epoch 8: val_loss=8.0312  val_ppl=3075.40\n",
      "epoch 9 step 100: LM=7.9693  Router(ce_align)=3.1487  H(p)=3.291\n",
      "==> epoch 9: val_loss=7.8354  val_ppl=2528.64\n",
      "epoch 10 step 100: LM=7.8844  Router(ce_align)=3.0688  H(p)=3.190\n",
      "==> epoch 10: val_loss=7.7137  val_ppl=2238.90\n",
      "epoch 11 step 100: LM=7.7892  Router(ce_align)=3.0897  H(p)=3.030\n",
      "==> epoch 11: val_loss=7.6297  val_ppl=2058.47\n",
      "epoch 12 step 100: LM=7.5307  Router(ce_align)=3.1201  H(p)=3.075\n",
      "==> epoch 12: val_loss=7.5203  val_ppl=1845.08\n",
      "epoch 13 step 100: LM=7.5264  Router(ce_align)=3.0773  H(p)=3.179\n",
      "==> epoch 13: val_loss=7.4798  val_ppl=1771.88\n",
      "epoch 14 step 100: LM=7.1870  Router(ce_align)=3.1657  H(p)=3.271\n",
      "==> epoch 14: val_loss=7.3953  val_ppl=1628.30\n",
      "epoch 15 step 100: LM=7.3340  Router(ce_align)=3.1671  H(p)=2.945\n",
      "==> epoch 15: val_loss=7.3684  val_ppl=1585.07\n"
     ]
    }
   ],
   "source": [
    "train_ST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d100c3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 100: LM=7.2707  Router(reinf)=0.0183  H(p)=3.101\n",
      "==> epoch 1: val_loss=7.3335  val_ppl=1530.80\n",
      "epoch 2 step 100: LM=7.3123  Router(reinf)=-0.1399  H(p)=2.731\n",
      "==> epoch 2: val_loss=7.2585  val_ppl=1420.12\n",
      "epoch 3 step 100: LM=7.2764  Router(reinf)=-0.0203  H(p)=2.723\n",
      "==> epoch 3: val_loss=7.1693  val_ppl=1298.96\n",
      "epoch 4 step 100: LM=7.4596  Router(reinf)=-0.2739  H(p)=0.054\n",
      "==> epoch 4: val_loss=7.1331  val_ppl=1252.82\n",
      "epoch 5 step 100: LM=7.3386  Router(reinf)=0.1113  H(p)=1.174\n",
      "==> epoch 5: val_loss=7.1089  val_ppl=1222.80\n",
      "epoch 6 step 100: LM=7.3158  Router(reinf)=-0.4142  H(p)=0.958\n",
      "==> epoch 6: val_loss=7.0553  val_ppl=1159.00\n",
      "epoch 7 step 100: LM=7.1677  Router(reinf)=-0.7611  H(p)=0.256\n",
      "==> epoch 7: val_loss=7.0355  val_ppl=1136.31\n",
      "epoch 8 step 100: LM=7.2777  Router(reinf)=-0.8712  H(p)=0.001\n",
      "==> epoch 8: val_loss=7.0051  val_ppl=1102.29\n",
      "epoch 9 step 100: LM=7.1225  Router(reinf)=0.0183  H(p)=1.189\n",
      "==> epoch 9: val_loss=6.9876  val_ppl=1083.15\n",
      "epoch 10 step 100: LM=7.2483  Router(reinf)=-0.5849  H(p)=0.000\n",
      "==> epoch 10: val_loss=6.9608  val_ppl=1054.46\n",
      "epoch 11 step 100: LM=7.1566  Router(reinf)=-0.3174  H(p)=0.000\n",
      "==> epoch 11: val_loss=6.9462  val_ppl=1039.16\n",
      "epoch 12 step 100: LM=7.0476  Router(reinf)=-0.7490  H(p)=0.301\n",
      "==> epoch 12: val_loss=6.9374  val_ppl=1030.04\n",
      "epoch 13 step 100: LM=7.1707  Router(reinf)=-0.9747  H(p)=0.000\n",
      "==> epoch 13: val_loss=6.9257  val_ppl=1018.12\n",
      "epoch 14 step 100: LM=7.0120  Router(reinf)=-0.8283  H(p)=0.042\n",
      "==> epoch 14: val_loss=6.9085  val_ppl=1000.75\n",
      "epoch 15 step 100: LM=7.0998  Router(reinf)=-0.7325  H(p)=0.000\n",
      "==> epoch 15: val_loss=6.8956  val_ppl=987.91\n"
     ]
    }
   ],
   "source": [
    "train_reinforce()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curriculum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
